\documentclass{article}

\usepackage{amsmath} % For gather and other math environments
\usepackage{setspace} % For double spacing
\usepackage{mathtools} % amsmath extensions
\usepackage{amsfonts} % math fonts
\usepackage{graphicx} % for images
\usepackage{listings} % for code blocks
\usepackage{pgfplots} % for plotting function
\usepackage{booktabs} % for tables

\usepackage[a4paper, margin=3.17cm]{geometry}

\graphicspath{{ ./images/ }}

% notes:
% https://hudsonthames.org/caveats-in-calibrating-the-ou-process/
% closed-form OU solution, do this youself tho: https://fxpaul.wordpress.com/2011/05/27/closed-form-solution-of-modified-ornstein-uhlenbeck-process/
% interesting, close form solution to SDE for trading: https://arxiv.org/pdf/2003.10502
% https://github.com/david-alber/Pairs-Trading-as-application-to-the-Ornstein-Uhlenbeck-Process

% process calibration:
% https://dr.lib.iastate.edu/entities/publication/bf86f169-61a5-44fc-8f2f-d8e6cd7e103a

% out-of-sample performance of an OU process:
% https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2720

% rly interesting:
% https://github.com/david-alber/Pairs-Trading-as-application-to-the-Ornstein-Uhlenbeck-Process/blob/master/Report%2BPoster/Pairs_Trading_Doc.pdf

% dickey fuller:
% https://www.youtube.com/watch?v=1opjnegd_hA

% source for unit-root testing (part of Engle Granger Test):
% https://pages.stern.nyu.edu/~churvich/Forecasting/Handouts/UnitRoot.pdf

% very good video on DF and ADF tests, doesn't actually really explain how to do them:
% https://www.youtube.com/watch?v=T5BhGv742j4

\begin{document}
\begin{spacing}{2.0}

\section{Brownian Motion and the Wiener Process}

Consider a stochastic process, where per each iteration, we add a $\Delta$ to the previous value,
this $\Delta$ is distributed by a normal distribution with a $mu$ of zero, and these are independent
stochastic events this is called the Wiener process. In discrete time, we can define the Wiener process
with the following equation
\begin{gather*}
    W_{t + 1} = W_{t} + \mathcal{N}(0, \sigma^{2})
\end{gather*}
Where $W_{t + 1}$ is the next value of the process (under discrete time), $\mathcal{N}(0, \sigma^{2})$
is a normally distributed drift variable with the variance $\sigma^{2}$.

Intuitively, the expected value, i.e. the limit of this process as it approaches infinity, should
be zero, since the normal distribution is symmetric. (maybe prove this? Using a limits on the $\mathbb{E}[W]$ and the normal dist)

The spread of the Wiener process grows at a rate of $\sigma_{t} = \sigma \sqrt{t}$, due to each timestep adding additional compounded uncertainty.

Here's an example of a Wiener process with $W_{0} = 0$ and $\sigma^{2} = 1$:
\begin{center}
    \includegraphics[scale=0.35]{./images/wiener.png}
\end{center}
In the diagram, we can clearly observe that the the variance of the Wiener process gets larger as the time progresses.

The wiener process is crucial and has many financial applications, for example, equity prices under the assumption of the efficient
market hypothesis follow a purely-stochastic Wiener process. This although disregards stochastic drift, which applies directional
pressure on equity prices, i.e., the Wiener process is market-agnostic, and applies to equity which do not follow the upward trend
of the market due to the positive risk-free rate.


\section{Ornstein-Uhlenbeck Process}

\subsection{Deriving a Mean Reversion Mechanism}

But what if we don't want the variance of the process to get larger as time progresses? We call these processes mean-reverting, as in they
tend to revert their values back towards the mean $\mu$. This has numerous financial applications (not to mention its applications outside
of finance), for example: statstical arbitrage, interest rate models, heston model, and mean reverting equities.

So how do we implement mean reversion? We can start by thinking about what mean reversion actually is? Mean reversion is the tendency of a
variable (or usually a stochastic process) to revert back to its long-term mean. If we consider the roots of brownian motion - physics -, we
can consider a mean-reverting term to be similar to a force pushing a particle back towards its mean; note that this doesn't happen
instantaneuously, it happens over a period of time, where the steadyness of this decay is dictated by the rate of mean-reversion, denoted as
$\theta$.

We know that an exponential function with a negative coefficient applied to the exponent will produce a function that will slowly decay.

\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        axis lines = left,
        xlabel = \(x\),
        ylabel = {\(f(x)\)},
    ]
        \addplot [
            domain=0:15,
            samples=100,
            color=red,
        ] {e^(-0.4 * x)};

        \addlegendentry{$f(x) = x_{0} \cdot e^{-\theta x}$}
    \end{axis}
\end{tikzpicture}
\end{center}

But this is not exactly mean reverting, we need to adjust the function to not only have the tendency to approach zero, but rather, the function must approach
the mean. And the function must be applied from any starting point $x_{0}$. Considering that $f(0) = x_{0}$, we can adjust the function and introduce a base
term: $f(x) = x_{0}e^{-\theta x}$, this ensures the initial value is $x_{0}$, since $e^{0} = 1$. Now we have to ensure that the function converges at the mean,
not at zero. Initially, we can try to add a the mean as a constant term, such that $f(x) = \mu + x_{0}e^{-\theta x}$, but when graphing this function, we notice
that we break our first condition - the initial value of the function $f(0)$ is not equal to $x_{0}$. We have to adjust for this in the coefficient of the
exponential term. We notice that the difference between $x_0$ and $f(0)$ is equal to the mean, thus modifying the function to the following will solve our problem:

\begin{gather*}
    f(x) = \mu + (x_{0} - \theta)e^{-\theta x}
\end{gather*}

The function produces the following graph, assuming $\mu = 4$, $\theta = 0.4$ and varying the initial value $x_{0}$ to be $x_{0} \in \{8, 5, -1 \} $ for each of the
separate lines respectivelly:

\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        axis lines = left,
        xlabel = \(x\),
        ylabel = {\(f(x)\)},
        legend style={
            at={(0.5,0.15)},
            anchor=north,
            cells={anchor=west}
        },
    ]
        \addplot [
            domain=0:20,
            samples=400,
            color=red,
        ] {4 + (1 - 4) * e^(-0.4 * x)};

        \addplot [
            domain=0:20,
            samples=400,
            color=green,
        ] {4 + (8 - 4) * e^(-0.4 * x)};

        \addplot [
            domain=0:20,
            samples=400,
            color=blue,
        ] {4 + (5 - 4) * e^(-0.4 * x)};

        \addlegendentry{$f(x) = \mu + (x_{0} - \mu) \cdot e^{-\theta x}$}
    \end{axis}
\end{tikzpicture}
\end{center}

\subsection{Differentiating for the Mean-reverting Force}

We can now think about creating a stochastic process that will take the Wiener process as a basis, but add mean-reverting properties in order to ensure
the variance stays within a certain range. Just adding a term for the brownian motion will not be enough, this would be misinterpreting the mean-reverting
effect and would be mostly useless for our purposses, thus we can't do something like:
\begin{gather*}
    X_{t} = \mu + (X_{0} - \mu) \cdot e^{-\theta t} + \sigma W_{t}
\end{gather*}
We must now calculate the tug or the actual amount by which the function changes per unit $t$. This will be necessary for our stochastic process, as they
are inherently defined in differential form (in descrete time), where the next value depends on the previous value and we simply add a modifying term, e.g.
$M_{t + 1} = M_{t} + \sigma$. We can do this by taking a simple derivative of the function. This derivative can then be thought of as the per-unit $t$ amount
of tug applied for each value above or below the mean.
\begin{gather*}
    \frac{d}{dt} \left ( \mu + \left ( X_{0} - \mu \right ) \cdot e^{-\theta t} \right ) \\
    \frac{d}{dt} \mu + \frac{d}{dt} \left ( X_{0} - \mu \right ) e^{-\theta t}
\end{gather*}
We know that the derivative of the constant $\mu$ is zero, thus we can eliminate the term. We are left with the derivative of a product, for which we can
use the product rule of differentiation: $\frac{d}{dt}x \cdot y = x'y + xy'$. In our case, the two terms are $( X_{0} - \mu )$ and $e^{-\theta t}$. The first
term is a constant that does not chage with respect to $t$, thus it will be zero. The second term can be differentiated using the chain rule.

We need to define an outer and an inner function in order to apply the chain rule, the inner function will be $e^{x}$ and the outer function will be $-\theta t$.
The $\frac{d}{dx}e^{x} = e^{x}$ is a standard differential. And the $\frac{d}{dx} -\theta x$ will simply collapse down to $-\theta$, since it's a constant that is
applied to our differentiating variable. The chain rule states:
\begin{gather*}
    \frac{d}{dx} f(g(x)) = f'(g(x)) \cdot g'(x) \\
    \frac{d}{dt} e^{-\theta x} = e^{-\theta x} \cdot (-\theta) = -\theta e^{-\theta x}
\end{gather*}
We can now revisit the prudct rule and apply it, we know that the derivative of our first term is zero, thus we can ignore the first term of the product rule.
Only leaving the following: $\frac{d}{dt}x \cdot y = xy'$, we have already calculated $y' = -\theta e^{-\theta x}$ in the previous step. Thus we can Just
multiply $-\theta e^{-\theta x}$ with $(X_{0} - \mu)$, and we get the following result for our whole derivative:
\begin{gather*}
    \frac{d}{dt} \left ( \mu + \left ( X_{0} - \mu \right ) \cdot e^{-\theta t} \right ) = (-\theta e^{-\theta x}) \cdot (X_{0} - \mu)
\end{gather*}
We can rearrange this to:
\begin{gather*}
    \frac{d}{dt} \left ( \mu + \left ( X_{0} - \mu \right ) \cdot e^{-\theta t} \right ) = -\theta (X_{0} - \mu) e^{-\theta t}
\end{gather*}
This differential desribes the mean-reverting tug applied to our brownian motion model.

\subsection{Deriving a full Ornstein Uhlenbeck Process}

NOT SURE IF THIS IS THE BEST WAY TO DO IT, MIGHT NOT BE IN THE FINAL IA.

We can now try to perform a few experiments on a naively-formulated OU (Ornstein Uhlenbeck) process. We can begin by simply adding a brownian motion to our stochastic
process, and assuming (naive step) $X_{0} = X_{t}$, since at each point, the differential will get us tug, this will mean that our $t$ is now zero.
We are left with the following stochastic process:
\begin{gather*}
    X_{t + 1} = -\theta(X_{t} - \mu)e^{-\theta \cdot 0} + W_{t} \\
    X_{t + 1} = -\theta(X_{t} - \mu)e + \mathcal{N}(0, \sigma^{2})
\end{gather*}
We can not create stochastic paths along the process and they should be mean reverting.
\begin{center}
    \includegraphics[scale=0.35]{./images/naive_ou.png}
\end{center}
Here we can see that the stochastic process is much more contained within a range, and is thus mean-reverting.

\section{Long-term Equilibrium Cointegration in Equity Markets}

We perform the \textbf{Two Step Engle-Granger Test} for cointegration of time series to get the cointegration coefficient. This method is widely accepted
in quantitative finance and econometrics.

But what is cointegration? Cointegration represents the level to which a linear combination
of two (or more) time series $X_{t} = \beta Y_{t}$ follows a stationary time series $I(0)$.
Or in other words, whether the linear combination has a zero long-term mean. We can take
any linear combination, such as the linear regression of $X_{t}$ on $Y_{t}$, in the next
sub-chapter, we form this regression and add an error term, the error term is esentially
formed from a linear combination of the two time series. A high cointegration coefficient
means that the two time series move in tandem with each other and that deviations in their
spread revert to the long-term mean (since the mean of the spread is zero), divergence of
spreads resulting in a zero long-term mean spread is impossible because the function is
continuous.

\subsection{Static Regression of Equity Time Series}

First, we need to determine the residuals $\hat \epsilon_{t}$ (similar to the error) of a regression between
the two time series of the prices of the equities we're analyzing. We define a linear regression $Y_{t} = \alpha + \beta X_{t} + \epsilon_{t}$.
Where $Y_{t}$ and $X_{t}$ are the $t$-th elements of the time series of the equity prices of $X$ and $Y$. And where $\alpha$ is the
intercept term (which is the mean level of $Y_{t}$ where $X_{t}$ is zero) and where $\beta$ is mean slope coefficient, i.e. the change in
the value of $Y_{t}$ per unit $X_{t}$.
We define the following intuitively or from the definition of the linear regression:
\begin{gather*}
    \beta = \frac{
        \sum_{i = 0}^{n}{(X_{i} - \mu_{X}) \cdot (Y_{i} - \mu_{Y})}
    }{
        \sum_{x \in X} (x - \mu_{X})^2
    }
    \\
    \alpha = \mu_{Y} - \beta \cdot \mu_{X}
\end{gather*}
Now we calculate the residuals $\hat \epsilon_{t}$ using the formula $\hat \epsilon_{t} = Y_{t} - (\alpha + \beta X_{t})$, which we obtain from rearranging the
terms of the linear regression equation with resduals. We can interpret these residuals as the difference (for each timestep $t$) between the actual value $X_{t}$
and the predicted value - the value of the regression solution for the parameter $t$.



\subsection{Dickey Fuller Test}

The Dickey Fuller Test or (DF Test for short) is a statistical test that tests for the stationarity of a time series, or in other words, it tests for if the
order of integration of the time series is $I(0)$. The non-augmented version of the Dickey Fuller Test is only applicable for an auto-regressive AR(1) process.
Equity spreads are widely considered to be AR(1) processes. We can define an AR(1) time series as $y_{t} = \alpha + \phi y_{t - 1} + \epsilon_{t}$, where $\alpha$ is
the constant shift term, $\phi y_{t - 1}$ is the autoregressive term where $\phi$ is a coefficient denoting how strongly the previous value of the time series
affects the current value at time $t$, and $\epsilon_{t}$ (not to be confused with the residuals from our regression model) denotes a random error term, which IS
assumed to be white noise $\epsilon_{t} \sim \mathcal{N}(0, \sigma^2)$. Now we can define the hypotheses for the DF Test.
\begin{gather*}
    H_{0} : \phi = 1 \\
    H_{1} : \phi < 1
\end{gather*}
Where the null hypothesis denotes that the process is non-stationary and the alternative hypothesis indicates that the process is stationary. We could simply use a t-test
to see if a $\hat \phi$ estimator is statistically significant, and based on that reject or fail to reject the null hypothesis of the DF Test. But the t-test is inapplicable
in models where the central limit theorem doesn't hold true (because the $\epsilon$ values of the process are not independent and identically distributed), thus we need to use the
DF test. First, we transform our original time series expression in as a differencing equation. Note that we also rewrite the $y_{t} - y_{t - 1}$ as $\Delta y_{t}$.
\begin{gather*}
    y_{t} = \alpha + \phi y_{t - 1} + \epsilon_{t} \\
    y_{t} - y_{t - 1} = \alpha + \phi y_{t - 1} - y_{t - 1} + \epsilon_{t} \\
    \Delta y_{t} = \alpha + y_{t - 1} (\phi - 1) + \epsilon_{t}
\end{gather*}
For the sake of simplicity, we introduce $\delta$ instead of $(\phi - 1)$. We can simplify our express to $\Delta y_{t} = \alpha + \delta y_{t - 1} + \epsilon_{t}$, which is
the standard form you'll find in most literature on the DF Test. Our hypotheses remain and can be rewritten in terms of $\delta$ instead of $\phi$.
\begin{gather*}
    H_{0} : \delta = 0 \\
    H_{1} : \delta < 0
\end{gather*}
We apply an Ordinary Least Squares linear regression to find the estimator $\hat \delta$, this is the p-value for our test and we compare it against our hypotheses. But in our case,
we this won't be rigorous enough since we didn't account for autocorrelation (In our case, this means that previous values have some sort of effect on the current value $y_{t}$ in our
time series) properties of financial time series, to remedy this and correctly account for autocorrelation, we use the Augmented Dickey Fuller Test or ADF for short.

\subsection{Augmented Dickey Fuller Test}

This is an extension of the Dickey Fuller Test that correctly accounts for autocorrelation between consecutive values of the time series and is also extended to the autoregressive
AR(p) process; it tests whether a time series of the order of integration $I(p)$ is stationary and for the presence of a unit root (which is a prerequisite of non-stationarity).
In the ADF test, our hypotheses stay the same.
\begin{gather*}
    H_{0} : \delta = 0 \text{ $\rightarrow$ non-stationary, unit root} \\
    H_{1} : \delta < 0 \text{ $\rightarrow$ stationary, no unit root}
\end{gather*}
We need to add lags to our expression to account for autocorrelation. We can model autocorrelation as some coefficient applied on some previous value of the time series also being
accounted in the value of the time series at $t$. Considering we add two lags to account for autocorrelation between the current value $y_{t}$ and values $y_{t - 1}$ and $y_{t - 2}$,
our expression will intuitively become $\Delta y_{t} = \alpha + y_{t - 1} (\phi - 1) + \beta_{1}(\Delta y_{t - 1}) + \beta_{2}(\Delta y_{t - 2}) + \epsilon_{t}$. Where the betas $\beta_{1}$
are our coefficients onf the lagged differences. We also define $p$ to be the number of lags we include, this value is automatically optimized in various statistical application
programming interfaces (APIs) and one can use various Bayesian estimation methods to get the optimal $p$ value, or one can perform a t-test (or an F-test when testing for the significance
of multiple variables) and keep adding lagged differences (thus increasing $p$) until the addition is no longer statistically significant. Our expression can be generalized in the
following form:
\begin{gather*}
    \Delta y_{t} = \alpha \delta y_{t - 1} + \sum_{i = 1}^{p} \beta_{i} \Delta y_{t - 1} + \epsilon_{t}
\end{gather*}
Now that we have this equation, we can do an Ordinary Least Squares regression (in its multiple linear regression form, since we have multiple regressors) to find
$\alpha, \delta, \beta_{1}, \beta_{2}, \cdots, \beta_{p}$, we then use the $\delta$ value as the final p-value for the Augmented Dickey Fuller Test.

\subsection{Performing the Two-Step Engle Granger Test on Equity Prices}

We now perform the two-step engle granger test on 78 mature stocks from the selected from NADSAQ-100 index from Janurary 1st 2014 to January 1st 2024. We pair every stock with every other
stock resulting in $78^2 = 6084$ pairs, for each of these pairs we perform an OLS regression and get the residual values (as described in subsection 1) and then perform the Augmented Dickey
Fuller Test using the 'statsmodels' library. We extract the p-values of the test and end up with the following heatmap:
\begin{center}
    \includegraphics[scale=0.45]{./images/cointegration.png}
\end{center}
Only 100 pairs are displayed as the complete 78x78 heatmap is too large to present here.

We now select 10 of the most cointegrated (lowest p-values) equity pairs to further examine, the ten most cointegrated pairs were, in descending order of cointegration, we observe
that most of the equity pairs with very heavy cointegration have AAPL (Apple Inc.) as the base stock, this is interesting as AAPL has heavily outperformed even the growth-heavy
NASDAQ, thus this should create a larger spread between most other stocks, suggesting limitations of our test.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Stock 1 Ticker} & \textbf{Stock 2 Ticker} & \textbf{ADF P-Value} \\ \hline
        AAPL & XRAY & 9.104296439449678e-05 \\ \hline
        AAPL & WBA & 0.00011254543577701014 \\ \hline
        AAPL & VRTX & 0.00014280647279306093 \\ \hline
        AAPL & VRSK & 0.00015429781044445213 \\ \hline
        AMGN & WBA & 0.00016469559752946386 \\ \hline
        AAPL & TSCO & 0.00016489122300362627 \\ \hline
        AAPL & WDC & 0.0001652537550668745 \\ \hline
        AAPL & TSLA & 0.00017722866444703876 \\ \hline
        AMGN & VOD & 0.00018081206670114813 \\ \hline
        AAPL & TXN & 0.00018223283943511726 \\ \hline
    \end{tabular}
\end{table}

For reference, we can plot the residuals of the initial regression of a select heavily cointegrated
pair and a non-cointegrated pair to better visualize the idea of cointegration.

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/amgn_vod_cointegration.png}
        \caption{AMGN, VOD, p-value: 0.000181}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/adbe_tsla_cointegration.png}
        \caption{ADBE, TSLA, p-value: 0.484251}
    \end{minipage}
\end{figure}

\subsection{Engle Granger Residuals vs Equity Spreads}

If we plot the residuals of AMGN and VOD, we can observe that the resultant time series clearly follows an OU process (or a time-discrete AR(1) time series), as is also indicated
by the test we performed, but if we plot the equity spreads (values of stock 1 minues values of stock 2), we observe a different pattern, indicating that our model is still incomplete
and that OLS residuals might not give us the full picture.




\section{Derivation of the Gradient for Non-linear Programming}

We optimize for the variable $\theta$. We get following base function.
\begin{gather*}
    X_{t + 1} = X_{t} + -\theta(X_{t} - \mu)e + \mathcal{N}(0, \sigma^{2}) \\
\end{gather*}
We must get the objective function which is thre mean squared error in terms of $\theta$.
We consider the the values to be in a pre-computed list $\mathcal{X}$, where $\mathcal{X}_{1}, \mathcal{X}_{2}, \cdots, \mathcal{X}_{n} \in \mathcal{X}$
\begin{gather*}
    MSE = \frac{1}{T} \sum_{i = 1}^{T} \mathcal{X}_{i} = \frac{1}{T} \sum_{i = 1}^{T} X_{i} + -\theta(X_{t} - \mu)e + \mathcal{N}(0, \sigma^{2})
\end{gather*}

\section{Least Squares Regression for Mean-reverting Parameter}
\begin{gather*}
    \mathcal{X}_{t + 1} = \mathcal{X}_{t} - \theta (X_{t} - \mu)e + \mathcal{N}(0, \sigma^{2}) \\
    \mathcal{X}_{t + 1} - \mathcal{X}_{t} - \mathcal{N}(0, \sigma^{2}) = -\theta (X_{t} - \mu)e  \\
    \frac{- \mathcal{X}_{t + 1} + \mathcal{X}_{t} + \mathcal{N}(0, \sigma^{2})}{e(X_{t} - \mu)} = \theta
\end{gather*}

For now, we can disregard the normally-distributed Wiener process term.
Have to use Ito's Lemma.

\end{spacing}
\end{document}
